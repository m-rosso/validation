{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation procedures\n",
    "## Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This projects conducted to the development of classes that have the goal of contributing with validation procedures during the implementation of data modeling in supervised learning tasks. This tutorial has the goal of showing its easy use and flexibility.\n",
    "<br>\n",
    "<br>\n",
    "Use cases for the classes presented here are as follows:\n",
    "* *KfoldsCV*, for perfoming grid/random search of a LightGBM model and a XGBoost model. Besides, pre-selection of features during each of the K-folds estimation for LightGBM is also presented.\n",
    "* *KfoldsCV_fit*, for performing grid/random search and fitting a SVM classifier using the entire training data and the best choices of hyper-parameters. Besides, the same for GBM classifier (sklearn) is applied together with parallelization for reducing overall running time. A demonstration of how to use this class for implementing XGBoost with early stopping is also available. Finally, logistic regression with pre-selection of features is demonstrated.\n",
    "* *BootstrapEstimation*, for running a large collection of estimations in order to assess average and standard deviation of performance metrics, using a regularized logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to notice that all estimations have no intention of being as efficient as possibile, but focus on illustrating how those classes can be used in real-world applications.\n",
    "<br>\n",
    "<br>\n",
    "The complete collection of learning algorithms covered by KfoldsCV, Kfolds_fit, and BootstrapEstimation classes are presented below. Each method is followed by the library of reference and the hyper-parameters subject to grid or random search. Note that all hyper-parameters are named exactly how they are in their original libraries.\n",
    "1. Logistic regression from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (method='logistic_regression').\n",
    "    * Main hyper-parameters for tuning: regularization parameter ('C').\n",
    "2. Linear regression (Lasso) from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) (method='lasso').\n",
    "    * Main hyper-parameters for tuning: regularization parameter ('C').\n",
    "3. GBM from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (method='gbm').\n",
    "    * Hyper-parameters for tuning: subsample ('subsample'), maximum depth ('max_depth'), learning rate ('learning_rate'), number of estimators ('n_estimators').\n",
    "4. GBM from [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html) (method='light_gbm').\n",
    "    * Main hyper-parameters for tuning: subsample ('bagging_fraction'), maximum depth ('max_depth'), learning rate ('learning_rate), number of estimators ('num_iterations').\n",
    "    * By declaring 'metric' and 'early_stopping_rounds' into the parameters dictionary, it is possible to implement both \"KfoldsCV\" and \"Kfolds_fit\" with early stopping. For \"KfoldsCV\", at each k-folds estimation early stopping will take place, while for \"Kfolds_fit\" estimation will stop after a stopping rule is triggered both during each of k-folds estimation and during the final fitting using the entire training data.\n",
    "5. GBM from [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html#xgboost-parameters) (method='xgboost').\n",
    "    * Main hyper-parameters for tuning: subsample ('subsample'), maximum depth ('max_depth'), learning rate ('eta'), number of estimators ('num_boost_round').\n",
    "    * By declaring 'eval_metric' and 'early_stopping_rounds' into the parameters dictionary, also for XGBoost early stopping is available for both \"KfoldsCV\" and \"Kfolds_fit\".\n",
    "6. Random forest from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) (method='random_forest').\n",
    "    * Main hyper-parameters for tuning: number of estimators ('n_estimators'), maximum number of features ('max_features') and minimum number of samplesfor split ('min_samples_split').\n",
    "7. SVM from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (method='svm').\n",
    "    * Main hyper-parameters for tuning: regularization parameter ('C') kernel ('kernel'), polynomial degree ('degree'), gamma ('gamma')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook imports the developed classes and uses a dataset for binary classification seeking to assess the functionalities of those classes by applying several distinct statistical learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "1. [Libraries](#libraries)<a href='#libraries'></a>.\n",
    "2. [Functions and classes](#functions_classes)<a href='#functions_classes'></a>.\n",
    "3. [Settings](#settings)<a href='#settings'></a>.\n",
    "4. [Importing datasets](#imports)<a href='#imports'></a>.\n",
    "5. [Data pre-processing](#data_pre_proc)<a href='#data_pre_proc'></a>.\n",
    "6. [Assessing K-folds CV](#kfolds_assess)<a href='#kfolds_assess'></a>.\n",
    "    * [LightGBM](#kfolds_lightgbm)<a href='#kfolds_lightgbm'></a>.\n",
    "    * [XGBoost](#kfolds_xgboost)<a href='#kfolds_xgboost'></a>.\n",
    "<br>\n",
    "<br>\n",
    "7. [Assessing K-folds fit](#kfolds_fit_assess)<a href='#kfolds_fit_assess'></a>.\n",
    "    * [SVM classifier](#kfolds_fit_svm_class)<a href='#kfolds_fit_svm_class'></a>.\n",
    "    * [Parallel estimation (GBM)](#kfolds_fit_gbm_parallel)<a href='#kfolds_fit_gbm_parallel'></a>.\n",
    "    * [XGBoost with early stopping](#kfolds_fit_xgboost_es)<a href='#kfolds_fit_xgboost_es'></a>.\n",
    "    * [Logistic regression with pre-selection of features](#kfolds_fit_lr_sel_feats)<a href='#kfolds_fit_lr_sel_feats'></a>.\n",
    "<br>\n",
    "<br>\n",
    "8. [Assessing bootstrap estimation](#boot_assess)<a href='#boot_assess'></a>.\n",
    "    * [Logistic regression](#boot_lr)<a href='#boot_lr'></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "from scipy.stats import uniform, norm, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='functions_classes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import loading_data, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfolds\n",
    "from kfolds import KfoldsCV, Kfolds_fit\n",
    "\n",
    "import bootstrap\n",
    "from bootstrap import BootstrapEstimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='settings'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset_id:\n",
    "dataset_id = 2706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feats_label'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\u001b[1mDataset 2706:\u001b[0m\n",
      "Shape of df: (7217, 1286).\n",
      "Number of distinct instances: 7217.\n",
      "Time period: from 2020-12-31 to 2021-02-17.\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------')\n",
    "print(f'\\033[1mDataset {dataset_id}:\\033[0m')\n",
    "\n",
    "df_train = loading_data(path=f'Datasets/dataset_{dataset_id}_train.csv',\n",
    "                        dtype={'order_id': str, 'store_id': int, 'epoch': str},\n",
    "                        id_var='order_id')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "# Accessory variables:\n",
    "drop_vars = ['y', 'order_id', 'epoch', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\u001b[1mDataset 2706:\u001b[0m\n",
      "Shape of df: (7217, 1286).\n",
      "Number of distinct instances: 7217.\n",
      "Time period: from 2021-02-17 to 2021-03-31.\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------')\n",
    "print(f'\\033[1mDataset {dataset_id}:\\033[0m')\n",
    "\n",
    "df_test = loading_data(path=f'Datasets/dataset_{dataset_id}_test.csv',\n",
    "                        dtype={'order_id': str, 'store_id': int, 'epoch': str},\n",
    "                        id_var='order_id')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_assess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing K-folds CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_lightgbm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](https://lightgbm.readthedocs.io/en/latest/index.html) for documentation of LightGBM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mK-folds CV outcomes:\u001b[0m\n",
      "Number of data folds: 3.\n",
      "Number of samples for random search: 10.\n",
      "Estimation method: light gbm.\n",
      "Metric for choosing best hyper-parameter: roc_auc.\n",
      "Best hyper-parameters: {'bagging_fraction': 0.6832649920297882, 'learning_rate': 0.08491106396508266, 'max_depth': 7, 'num_iterations': 100}.\n",
      "CV performance metric associated with best hyper-parameters: 0.9825.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 0.68 minutes.\n",
      "Start time: 2021-07-18, 13:29:08\n",
      "End time: 2021-07-18, 13:29:49\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'bagging_fraction': uniform(0.5, 0.5),\n",
    "              'learning_rate': uniform(0.0001, 0.1),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'num_iterations': [100, 250, 500]}\n",
    "default_param = {'bagging_fraction': 0.75, 'learning_rate': 0.01, 'max_depth': 10, 'num_iterations': 500}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = KfoldsCV(task='binary', method='light_gbm', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                  random_search=True, n_samples=10,\n",
    "                  grid_param=grid_param, default_param=default_param,\n",
    "                  pre_selecting=False,\n",
    "                  parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.run(inputs=df_train.drop(drop_vars, axis=1), output=df_train['y'])\n",
    "\n",
    "# Defining best tuning hyper-parameter:\n",
    "best_param = kfolds.best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6832649920297882,\n",
       " 'learning_rate': 0.08491106396508266,\n",
       " 'max_depth': 7,\n",
       " 'num_iterations': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best tuning hyper-parameters:\n",
    "kfolds.best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8a57d_row0_col0, #T_8a57d_row1_col0, #T_8a57d_row2_col0, #T_8a57d_row3_col0, #T_8a57d_row4_col0, #T_8a57d_row5_col0, #T_8a57d_row6_col0, #T_8a57d_row7_col0, #T_8a57d_row8_col0, #T_8a57d_row9_col0 {\n",
       "  width: 300px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8a57d_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >tun_param</th>\n",
       "      <th class=\"col_heading level0 col1\" >cv_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_8a57d_row0_col0\" class=\"data row0 col0\" >{'bagging_fraction': 0.6832649920297882, 'learning_rate': 0.08491106396508266, 'max_depth': 7, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row0_col1\" class=\"data row0 col1\" >0.982525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "      <td id=\"T_8a57d_row1_col0\" class=\"data row1 col0\" >{'bagging_fraction': 0.8608926606533922, 'learning_rate': 0.04263524339823467, 'max_depth': 9, 'num_iterations': 250}</td>\n",
       "      <td id=\"T_8a57d_row1_col1\" class=\"data row1 col1\" >0.980851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row2\" class=\"row_heading level0 row2\" >7</th>\n",
       "      <td id=\"T_8a57d_row2_col0\" class=\"data row2 col0\" >{'bagging_fraction': 0.720694670763939, 'learning_rate': 0.03540258497810973, 'max_depth': 8, 'num_iterations': 250}</td>\n",
       "      <td id=\"T_8a57d_row2_col1\" class=\"data row2 col1\" >0.980705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row3\" class=\"row_heading level0 row3\" >6</th>\n",
       "      <td id=\"T_8a57d_row3_col0\" class=\"data row3 col0\" >{'bagging_fraction': 0.8655896154122709, 'learning_rate': 0.07974806260814968, 'max_depth': 5, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row3_col1\" class=\"data row3 col1\" >0.980403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row4\" class=\"row_heading level0 row4\" >8</th>\n",
       "      <td id=\"T_8a57d_row4_col0\" class=\"data row4 col0\" >{'bagging_fraction': 0.8324012409439627, 'learning_rate': 0.08192613707707953, 'max_depth': 2, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row4_col1\" class=\"data row4 col1\" >0.980156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row5\" class=\"row_heading level0 row5\" >2</th>\n",
       "      <td id=\"T_8a57d_row5_col0\" class=\"data row5 col0\" >{'bagging_fraction': 0.513212829128326, 'learning_rate': 0.02947968410710311, 'max_depth': 5, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row5_col1\" class=\"data row5 col1\" >0.978885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row6\" class=\"row_heading level0 row6\" >3</th>\n",
       "      <td id=\"T_8a57d_row6_col0\" class=\"data row6 col0\" >{'bagging_fraction': 0.6725562945020598, 'learning_rate': 0.02627670694304666, 'max_depth': 7, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row6_col1\" class=\"data row6 col1\" >0.977479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row7\" class=\"row_heading level0 row7\" >9</th>\n",
       "      <td id=\"T_8a57d_row7_col0\" class=\"data row7 col0\" >{'bagging_fraction': 0.7564047931344857, 'learning_rate': 0.005768237590992054, 'max_depth': 5, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row7_col1\" class=\"data row7 col1\" >0.966220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row8\" class=\"row_heading level0 row8\" >4</th>\n",
       "      <td id=\"T_8a57d_row8_col0\" class=\"data row8 col0\" >{'bagging_fraction': 0.5503319176774703, 'learning_rate': 0.011888356094332886, 'max_depth': 2, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row8_col1\" class=\"data row8 col1\" >0.964302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8a57d_level0_row9\" class=\"row_heading level0 row9\" >5</th>\n",
       "      <td id=\"T_8a57d_row9_col0\" class=\"data row9 col0\" >{'bagging_fraction': 0.80911447600028, 'learning_rate': 0.003551497272993853, 'max_depth': 5, 'num_iterations': 100}</td>\n",
       "      <td id=\"T_8a57d_row9_col1\" class=\"data row9 col1\" >0.962228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2de937ae50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV metrics:\n",
    "kfolds.CV_metric.sort_values('cv_roc_auc',\n",
    "                             ascending=False).style.set_properties(subset=['tun_param'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [                                      ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [---                                   ]  10%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 216 were selected!\n",
      "From 1282 features, 211 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 200 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [-------                               ]  20%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "From 1282 features, 213 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 203 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [-----------                           ]  30%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 200 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------                       ]  40%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 198 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 219 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [-------------------                   ]  50%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------                ]  60%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 209 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 202 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------            ]  70%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "From 1282 features, 209 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 219 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [------------------------------        ]  80%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 208 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 200 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 215 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------------------    ]  90%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 198 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "---------------------------------------------------------------------\n",
      "\u001b[1mK-folds CV outcomes:\u001b[0m\n",
      "Number of data folds: 3.\n",
      "Number of samples for random search: 10.\n",
      "Estimation method: light gbm.\n",
      "Metric for choosing best hyper-parameter: roc_auc.\n",
      "Best hyper-parameters: {'bagging_fraction': 0.6393204625867241, 'learning_rate': 0.06452780151570851, 'max_depth': 6, 'num_iterations': 100}.\n",
      "CV performance metric associated with best hyper-parameters: 0.9832.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 0.86 minutes.\n",
      "Start time: 2021-07-18, 13:30:52\n",
      "End time: 2021-07-18, 13:31:44\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'bagging_fraction': uniform(0.5, 0.5),\n",
    "              'learning_rate': uniform(0.0001, 0.1),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'num_iterations': [100, 250, 500]}\n",
    "default_param = {'bagging_fraction': 0.75, 'learning_rate': 0.01, 'max_depth': 10, 'num_iterations': 500}\n",
    "\n",
    "# Parameters for features selection:\n",
    "selection_params = {\n",
    "    'method': 'supervised', 'threshold': 0,\n",
    "    'estimator': LogisticRegression(C=1.0, penalty='l1', solver='liblinear')\n",
    "}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = KfoldsCV(task='binary', method='light_gbm', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                  random_search=True, n_samples=10,\n",
    "                  grid_param=grid_param, default_param=default_param,\n",
    "                  pre_selecting=True, pre_selecting_params=selection_params,\n",
    "                  parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.run(inputs=df_train.drop(drop_vars, axis=1), output=df_train['y'])\n",
    "\n",
    "# Defining best tuning hyper-parameter:\n",
    "best_param = kfolds.best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping during each of the K-folds estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [                                      ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 200 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [---                                   ]  10%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 207 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 198 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [-------                               ]  20%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 202 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [-----------                           ]  30%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 212 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------                       ]  40%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "From 1282 features, 212 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 197 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [-------------------                   ]  50%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 212 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 202 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------                ]  60%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------            ]  70%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "From 1282 features, 214 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 199 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [------------------------------        ]  80%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 218 were selected!\n",
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------------------    ]  90%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 219 were selected!\n",
      "From 1282 features, 210 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 202 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 218 were selected!\n",
      "---------------------------------------------------------------------\n",
      "\u001b[1mK-folds CV outcomes:\u001b[0m\n",
      "Number of data folds: 3.\n",
      "Number of samples for random search: 10.\n",
      "Estimation method: light gbm.\n",
      "Metric for choosing best hyper-parameter: roc_auc.\n",
      "Best hyper-parameters: {'bagging_fraction': 0.5780735721079513, 'learning_rate': 0.06945847301187266, 'max_depth': 4, 'num_iterations': 250, 'early_stopping_rounds': 20, 'metric': 'auc'}.\n",
      "CV performance metric associated with best hyper-parameters: 0.9822.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 0.73 minutes.\n",
      "Start time: 2021-07-18, 13:31:44\n",
      "End time: 2021-07-18, 13:32:27\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'bagging_fraction': uniform(0.5, 0.5),\n",
    "              'learning_rate': uniform(0.0001, 0.1),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'num_iterations': [250],\n",
    "              'early_stopping_rounds': [20],\n",
    "              'metric': ['auc']\n",
    "             }\n",
    "default_param = {'bagging_fraction': 0.75, 'learning_rate': 0.01, 'max_depth': 10, 'num_iterations': 500}\n",
    "\n",
    "# Parameters for features selection:\n",
    "selection_params = {\n",
    "    'method': 'supervised', 'threshold': 0,\n",
    "    'estimator': LogisticRegression(C=1.0, penalty='l1', solver='liblinear')\n",
    "}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = KfoldsCV(task='binary', method='light_gbm', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                  random_search=True, n_samples=10,\n",
    "                  grid_param=grid_param, default_param=default_param,\n",
    "                  pre_selecting=True, pre_selecting_params=selection_params,\n",
    "                  parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.run(inputs=df_train.drop(drop_vars, axis=1), output=df_train['y'])\n",
    "\n",
    "# Defining best tuning hyper-parameter:\n",
    "best_param = kfolds.best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_xgboost'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](https://xgboost.readthedocs.io/en/latest/index.html) for documentation of XGBoost library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [                                      ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:32:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:32:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [---                                   ]  10%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:33:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:33:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-------                               ]  20%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:34:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:34:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:34:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-----------                           ]  30%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:35:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:35:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:36:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------                       ]  40%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:36:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:37:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:37:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-------------------                   ]  50%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:38:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:38:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------                ]  60%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:38:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:39:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------            ]  70%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:39:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:39:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:39:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [------------------------------        ]  80%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:39:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:40:02] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:40:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------------------    ]  90%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:40:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:40:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:40:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mK-folds CV outcomes:\u001b[0m\n",
      "Number of data folds: 3.\n",
      "Number of samples for random search: 10.\n",
      "Estimation method: xgboost.\n",
      "Metric for choosing best hyper-parameter: roc_auc.\n",
      "Best hyper-parameters: {'subsample': 0.9996099353984378, 'eta': 0.05260007383864195, 'max_depth': 3, 'num_boost_round': 250}.\n",
      "CV performance metric associated with best hyper-parameters: 0.9809.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 8.6 minutes.\n",
      "Start time: 2021-07-18, 13:32:27\n",
      "End time: 2021-07-18, 13:41:03\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'subsample': uniform(0.5, 0.5),\n",
    "              'eta': uniform(0.0001, 0.1),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'num_boost_round': [100, 250, 500]}\n",
    "default_param = {'subsample': 0.75, 'eta': 0.01, 'max_depth': 10, 'num_boost_round': 100}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = KfoldsCV(task='binary:logistic', method='xgboost', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                  random_search=True, n_samples=10,\n",
    "                  grid_param=grid_param, default_param=default_param,\n",
    "                  pre_selecting=False,\n",
    "                  parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.run(inputs=df_train.drop(drop_vars, axis=1), output=df_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2d7c3_row0_col0, #T_2d7c3_row1_col0, #T_2d7c3_row2_col0, #T_2d7c3_row3_col0, #T_2d7c3_row4_col0, #T_2d7c3_row5_col0, #T_2d7c3_row6_col0, #T_2d7c3_row7_col0, #T_2d7c3_row8_col0, #T_2d7c3_row9_col0 {\n",
       "  width: 300px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2d7c3_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >tun_param</th>\n",
       "      <th class=\"col_heading level0 col1\" >cv_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row0\" class=\"row_heading level0 row0\" >7</th>\n",
       "      <td id=\"T_2d7c3_row0_col0\" class=\"data row0 col0\" >{'subsample': 0.9996099353984378, 'eta': 0.05260007383864195, 'max_depth': 3, 'num_boost_round': 250}</td>\n",
       "      <td id=\"T_2d7c3_row0_col1\" class=\"data row0 col1\" >0.980909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
       "      <td id=\"T_2d7c3_row1_col0\" class=\"data row1 col0\" >{'subsample': 0.7625820775305374, 'eta': 0.04381120313104465, 'max_depth': 8, 'num_boost_round': 500}</td>\n",
       "      <td id=\"T_2d7c3_row1_col1\" class=\"data row1 col1\" >0.980256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row2\" class=\"row_heading level0 row2\" >8</th>\n",
       "      <td id=\"T_2d7c3_row2_col0\" class=\"data row2 col0\" >{'subsample': 0.9983049580837712, 'eta': 0.04993105449021878, 'max_depth': 9, 'num_boost_round': 250}</td>\n",
       "      <td id=\"T_2d7c3_row2_col1\" class=\"data row2 col1\" >0.979666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row3\" class=\"row_heading level0 row3\" >2</th>\n",
       "      <td id=\"T_2d7c3_row3_col0\" class=\"data row3 col0\" >{'subsample': 0.5530401948568808, 'eta': 0.0667633300311981, 'max_depth': 5, 'num_boost_round': 500}</td>\n",
       "      <td id=\"T_2d7c3_row3_col1\" class=\"data row3 col1\" >0.978711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
       "      <td id=\"T_2d7c3_row4_col0\" class=\"data row4 col0\" >{'subsample': 0.798463742093148, 'eta': 0.06221069321952554, 'max_depth': 3, 'num_boost_round': 250}</td>\n",
       "      <td id=\"T_2d7c3_row4_col1\" class=\"data row4 col1\" >0.978672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_2d7c3_row5_col0\" class=\"data row5 col0\" >{'subsample': 0.7826607740727265, 'eta': 0.09324628928635169, 'max_depth': 9, 'num_boost_round': 250}</td>\n",
       "      <td id=\"T_2d7c3_row5_col1\" class=\"data row5 col1\" >0.978445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row6\" class=\"row_heading level0 row6\" >3</th>\n",
       "      <td id=\"T_2d7c3_row6_col0\" class=\"data row6 col0\" >{'subsample': 0.9475319617512563, 'eta': 0.06452459453363402, 'max_depth': 5, 'num_boost_round': 500}</td>\n",
       "      <td id=\"T_2d7c3_row6_col1\" class=\"data row6 col1\" >0.978408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row7\" class=\"row_heading level0 row7\" >1</th>\n",
       "      <td id=\"T_2d7c3_row7_col0\" class=\"data row7 col0\" >{'subsample': 0.941260472119148, 'eta': 0.07852820147476218, 'max_depth': 4, 'num_boost_round': 500}</td>\n",
       "      <td id=\"T_2d7c3_row7_col1\" class=\"data row7 col1\" >0.978156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_2d7c3_row8_col0\" class=\"data row8 col0\" >{'subsample': 0.6372518708338644, 'eta': 0.07321161675319736, 'max_depth': 6, 'num_boost_round': 100}</td>\n",
       "      <td id=\"T_2d7c3_row8_col1\" class=\"data row8 col1\" >0.976528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d7c3_level0_row9\" class=\"row_heading level0 row9\" >6</th>\n",
       "      <td id=\"T_2d7c3_row9_col0\" class=\"data row9 col0\" >{'subsample': 0.6794803876204119, 'eta': 0.06437992563757859, 'max_depth': 1, 'num_boost_round': 250}</td>\n",
       "      <td id=\"T_2d7c3_row9_col1\" class=\"data row9 col1\" >0.976426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2de93465d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV metrics:\n",
    "kfolds.CV_metric.sort_values('cv_roc_auc',\n",
    "                             ascending=False).style.set_properties(subset=['tun_param'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping during each of the K-folds estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mK-folds CV outcomes:\u001b[0m\n",
      "Number of data folds: 3.\n",
      "Number of samples for random search: 10.\n",
      "Estimation method: xgboost.\n",
      "Metric for choosing best hyper-parameter: roc_auc.\n",
      "Best hyper-parameters: {'subsample': 0.9453305466279336, 'eta': 0.06991058585004947, 'max_depth': 8, 'num_boost_round': 500, 'early_stopping_rounds': 20, 'eval_metric': 'auc'}.\n",
      "CV performance metric associated with best hyper-parameters: 0.9805.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 2.49 minutes.\n",
      "Start time: 2021-07-18, 13:41:03\n",
      "End time: 2021-07-18, 13:43:33\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'subsample': uniform(0.5, 0.5),\n",
    "              'eta': uniform(0.0001, 0.1),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'num_boost_round': [100, 250, 500],\n",
    "              'early_stopping_rounds': [20],\n",
    "              'eval_metric': ['auc']\n",
    "             }\n",
    "default_param = {'subsample': 0.75, 'eta': 0.01, 'max_depth': 10, 'num_boost_round': 100}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = KfoldsCV(task='binary:logistic', method='xgboost', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                  random_search=True, n_samples=10,\n",
    "                  grid_param=grid_param, default_param=default_param,\n",
    "                  pre_selecting=False,\n",
    "                  parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.run(inputs=df_train.drop(drop_vars, axis=1), output=df_train['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_fit_assess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing K-folds fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_fit_svm_class'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: SVM.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'C': 1, 'kernel': 'poly', 'degree': 1, 'gamma': 'scale'}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9639.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9833\n",
      "   test_prec_avg = 0.9236\n",
      "   test_brier = 0.0091\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 1.74 minutes.\n",
      "Start time: 2021-07-18, 13:44:35\n",
      "End time: 2021-07-18, 13:46:19\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Declare grid of hyper-parameters:\n",
    "params = {'C': [1],\n",
    "          'kernel': ['poly'],\n",
    "          'degree': [1, 2, 3, 4],\n",
    "          'gamma': ['scale']}\n",
    "params_default = {'C': 1.0, 'kernel': 'poly', 'degree': 1, 'gamma': 'scale'}\n",
    "fixed_params = {'probability': True}\n",
    "\n",
    "# Declare K-folds CV estimation object:\n",
    "kfolds = Kfolds_fit(task='classification', method='SVM',\n",
    "                    metric='roc_auc', num_folds=3, random_search=False, shuffle=False,\n",
    "                    grid_param=params, default_param=params_default, fixed_params=fixed_params,\n",
    "                    pre_selecting=False,\n",
    "                    parallelize=False)\n",
    "\n",
    "# Running train-test estimation:\n",
    "kfolds.fit(train_inputs=df_train.drop(drop_vars, axis=1),\n",
    "           train_output=df_train['y'],\n",
    "           test_inputs=df_test.drop(drop_vars, axis=1),\n",
    "           test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_fit_gbm_parallel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel estimation (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential train-validation estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: GBM.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'subsample': 0.75, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9609.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9904\n",
      "   test_prec_avg = 0.9484\n",
      "   test_brier = 0.0043\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 41.26 minutes.\n",
      "Start time: 2021-07-18, 13:48:03\n",
      "End time: 2021-07-18, 14:29:19\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Declare grid of hyper-parameters:\n",
    "params = {'subsample': [0.75],\n",
    "          'learning_rate': [0.0001, 0.001, 0.01],\n",
    "          'max_depth': [1, 3, 5],\n",
    "          'n_estimators': [500]}\n",
    "params_default = {'subsample': 0.75,\n",
    "                  'learning_rate': 0.01,\n",
    "                  'max_depth': 10,\n",
    "                  'n_estimators': 500}\n",
    "fixed_params = {'warm_start':True}\n",
    "\n",
    "# Declare K-folds CV estimation object:\n",
    "train_test_est = Kfolds_fit(task='classification', method='GBM', metric='roc_auc', num_folds=3, shuffle=False,\n",
    "                            random_search=False,\n",
    "                            grid_param=params, default_param=params_default, fixed_params=fixed_params,\n",
    "                            pre_selecting=False,\n",
    "                            parallelize=False)\n",
    "\n",
    "# Running train-test estimation:\n",
    "train_test_est.fit(train_inputs=df_train.drop(drop_vars, axis=1),\n",
    "                   train_output=df_train['y'],\n",
    "                   test_inputs=df_test.drop(drop_vars, axis=1),\n",
    "                   test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel train-validation estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: GBM.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'subsample': 0.75, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9601.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9859\n",
      "   test_prec_avg = 0.9481\n",
      "   test_brier = 0.0043\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 17.41 minutes.\n",
      "Start time: 2021-07-18, 14:33:32\n",
      "End time: 2021-07-18, 14:50:57\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Declare grid of hyper-parameters:\n",
    "params = {'subsample': [0.75],\n",
    "          'learning_rate': [0.0001, 0.001, 0.01],\n",
    "          'max_depth': [1, 3, 5],\n",
    "          'n_estimators': [500]}\n",
    "params_default = {'subsample': 0.75,\n",
    "                  'learning_rate': 0.01,\n",
    "                  'max_depth': 10,\n",
    "                  'n_estimators': 500}\n",
    "fixed_params = {'warm_start':True}\n",
    "\n",
    "# Declare K-folds CV estimation object:\n",
    "train_test_est = Kfolds_fit(task='classification', method='GBM', metric='roc_auc', num_folds=3, shuffle=False,\n",
    "                            random_search=False,\n",
    "                            pre_selecting=False,\n",
    "                            grid_param=params, default_param=params_default, fixed_params=fixed_params,\n",
    "                            parallelize=True)\n",
    "\n",
    "# Running train-test estimation:\n",
    "train_test_est.fit(train_inputs=df_train.drop(drop_vars, axis=1),\n",
    "                   train_output=df_train['y'],\n",
    "                   test_inputs=df_test.drop(drop_vars, axis=1),\n",
    "                   test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_fit_xgboost_es'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [                                      ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:50:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:51:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:51:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----                                  ]  11%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:51:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:51:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:51:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------                              ]  22%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:51:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:51:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:52:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [------------                          ]  33%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:52:15] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:52:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:52:23] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------                      ]  44%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:52:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:52:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:52:45] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------------                 ]  55%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:52:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:53:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:53:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-------------------------             ]  66%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:53:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:53:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:53:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-----------------------------         ]  77%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:53:47] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:53:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:54:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------------------------     ]  88%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:54:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:54:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:54:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:54:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: xgboost.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'subsample': 0.75, 'eta': 0.1, 'max_depth': 5, 'num_boost_round': 200}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9793.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9912\n",
      "   test_prec_avg = 0.9611\n",
      "   test_brier = 0.0047\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 4.15 minutes.\n",
      "Start time: 2021-07-18, 14:50:57\n",
      "End time: 2021-07-18, 14:55:05\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'subsample': [0.75],\n",
    "              'eta': [0.001, 0.01, 0.1],\n",
    "              'max_depth': [1, 3, 5],\n",
    "              'num_boost_round': [200]}\n",
    "default_param = {'subsample': 0.75, 'eta': 0.01, 'max_depth': 10, 'num_boost_round': 100}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = Kfolds_fit(task='binary:logistic', method='xgboost', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                    random_search=False,\n",
    "                    pre_selecting=False,\n",
    "                    grid_param=grid_param, default_param=default_param,\n",
    "                    parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.fit(train_inputs=df_train.drop(drop_vars, axis=1), train_output=df_train['y'],\n",
    "           test_inputs=df_test.drop(drop_vars, axis=1), test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n",
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: xgboost.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'subsample': 0.75, 'eta': 0.1, 'max_depth': 5, 'num_boost_round': 200, 'eval_metric': 'auc', 'early_stopping_rounds': 20}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.981.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9936\n",
      "   test_prec_avg = 0.9608\n",
      "   test_brier = 0.0044\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 1.33 minutes.\n",
      "Start time: 2021-07-18, 14:55:05\n",
      "End time: 2021-07-18, 14:56:25\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus_rosso/fsvenv/lib/python3.7/site-packages/xgboost/core.py:104: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'subsample': [0.75],\n",
    "              'eta': [0.001, 0.01, 0.1],\n",
    "              'max_depth': [1, 3, 5],\n",
    "              'num_boost_round': [200],\n",
    "              'eval_metric': ['auc'],\n",
    "              'early_stopping_rounds': [20]}\n",
    "default_param = {'subsample': 0.75, 'eta': 0.01, 'max_depth': 10, 'num_boost_round': 100}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = Kfolds_fit(task='binary:logistic', method='xgboost', num_folds=3, metric='roc_auc', shuffle=False,\n",
    "                    random_search=False,\n",
    "                    pre_selecting=False,\n",
    "                    grid_param=grid_param, default_param=default_param,\n",
    "                    parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.fit(train_inputs=df_train.drop(drop_vars, axis=1), train_output=df_train['y'],\n",
    "           val_inputs=df_test.drop(drop_vars, axis=1), val_output=df_test['y'],\n",
    "           test_inputs=df_test.drop(drop_vars, axis=1), test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds_fit_lr_sel_feats'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with pre-selection of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [                                      ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 208 were selected!\n",
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--                                    ]   7%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 213 were selected!\n",
      "From 1282 features, 211 were selected!\n",
      "From 1282 features, 201 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-----                                 ]  14%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 217 were selected!\n",
      "From 1282 features, 211 were selected!\n",
      "From 1282 features, 200 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------                              ]  21%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 218 were selected!\n",
      "From 1282 features, 212 were selected!\n",
      "From 1282 features, 199 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----------                            ]  28%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 215 were selected!\n",
      "From 1282 features, 210 were selected!\n",
      "From 1282 features, 199 were selected!\n",
      "From 1282 features, 219 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-------------                         ]  35%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n",
      "From 1282 features, 198 were selected!\n",
      "From 1282 features, 219 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------                      ]  42%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n",
      "From 1282 features, 201 were selected!\n",
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-------------------                   ]  50%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 209 were selected!\n",
      "From 1282 features, 202 were selected!\n",
      "From 1282 features, 216 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------------                 ]  57%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 213 were selected!\n",
      "From 1282 features, 200 were selected!\n",
      "From 1282 features, 217 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [------------------------              ]  64%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 208 were selected!\n",
      "From 1282 features, 199 were selected!\n",
      "From 1282 features, 220 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [---------------------------           ]  71%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 206 were selected!\n",
      "From 1282 features, 202 were selected!\n",
      "From 1282 features, 215 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-----------------------------         ]  78%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n",
      "From 1282 features, 201 were selected!\n",
      "From 1282 features, 217 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------      ]  85%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 211 were selected!\n",
      "From 1282 features, 202 were selected!\n",
      "From 1282 features, 213 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [-----------------------------------   ]  92%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 214 were selected!\n",
      "From 1282 features, 201 were selected!\n",
      "From 1282 features, 217 were selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 277 were selected!\n",
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: logistic regression.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'C': 0.1}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9719.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9864\n",
      "   test_prec_avg = 0.9327\n",
      "   test_brier = 0.0087\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 1.7 minutes.\n",
      "Start time: 2021-07-18, 14:56:25\n",
      "End time: 2021-07-18, 14:58:07\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'C': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.25, 0.3, 0.5, 0.75, 1, 3, 10]}\n",
    "default_param = {'C': 1.0}\n",
    "fixed_params = {'penalty':'l1', 'solver':'liblinear', 'warm_start':True}\n",
    "\n",
    "# Parameters for features selection:\n",
    "selection_params = {\n",
    "    'method': 'supervised', 'threshold': 0,\n",
    "    'estimator': LogisticRegression(C=1.0, penalty='l1', solver='liblinear')\n",
    "}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = Kfolds_fit(task='classification', method='logistic_regression', num_folds=3, metric='roc_auc',\n",
    "                    shuffle=False,\n",
    "                    random_search=False,\n",
    "                    grid_param=grid_param, default_param=default_param, fixed_params=fixed_params,\n",
    "                    pre_selecting=True, pre_selecting_params=selection_params, only_final_selection=False,\n",
    "                    parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.fit(train_inputs=df_train.drop(drop_vars, axis=1), train_output=df_train['y'],\n",
    "           test_inputs=df_test.drop(drop_vars, axis=1), test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [--------------------------------------] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1282 features, 278 were selected!\n",
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Estimation method: logistic regression.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'C': 0.1}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9723.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9864\n",
      "   test_prec_avg = 0.9327\n",
      "   test_brier = 0.0087\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 0.71 minutes.\n",
      "Start time: 2021-07-18, 14:58:07\n",
      "End time: 2021-07-18, 14:58:50\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'C': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.25, 0.3, 0.5, 0.75, 1, 3, 10]}\n",
    "default_param = {'C': 1.0}\n",
    "fixed_params = {'penalty':'l1', 'solver':'liblinear', 'warm_start':True}\n",
    "\n",
    "# Parameters for features selection:\n",
    "selection_params = {\n",
    "    'method': 'supervised', 'threshold': 0,\n",
    "    'estimator': LogisticRegression(C=1.0, penalty='l1', solver='liblinear')\n",
    "}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = Kfolds_fit(task='classification', method='logistic_regression', num_folds=3, metric='roc_auc',\n",
    "                    shuffle=False,\n",
    "                    random_search=False,\n",
    "                    grid_param=grid_param, default_param=default_param, fixed_params=fixed_params,\n",
    "                    pre_selecting=True, pre_selecting_params=selection_params, only_final_selection=True,\n",
    "                    parallelize=False)\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.fit(train_inputs=df_train.drop(drop_vars, axis=1), train_output=df_train['y'],\n",
    "           test_inputs=df_test.drop(drop_vars, axis=1), test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boot_assess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing bootstrap estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boot_lr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBoostrap estimation progress: \u001b[0m[==================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "\u001b[1mBootstrap statistics:\u001b[0m\n",
      "   Number of estimations: 1000.\n",
      "   avg(roc_auc) = 0.9841\n",
      "   std(roc_auc) = 0.0017\n",
      "   avg(prec_avg) = 0.9238\n",
      "   std(prec_avg) = 0.005\n",
      "   avg(brier) = 0.0095\n",
      "   std(brier) = 0.0005\n",
      "\n",
      "\n",
      "\u001b[1m   Performance metrics based on bootstrap scores:\u001b[0m\n",
      "   roc_auc = 0.9865\n",
      "   prec_avg = 0.9334\n",
      "   brier = 0.0088\n",
      "   Hyper-parameters used in estimations: {'C': 0.1}.\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 19.45 minutes.\n",
      "Start time: 2021-07-18, 14:58:50\n",
      "End time: 2021-07-18, 15:18:17\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Declare grid of hyper-parameters:\n",
    "params = {'C': [0.1]}\n",
    "params_default = {'C': 0.1}\n",
    "fixed_params = {'penalty':'l1', 'solver':'liblinear', 'warm_start':True}\n",
    "\n",
    "# Declare bootstrap estimation object:\n",
    "boot_estimations = BootstrapEstimation(task='classification', method='logistic_regression',\n",
    "                                       metric='roc_auc', num_folds=3, shuffle=False,\n",
    "                                       pre_selecting=False,\n",
    "                                       random_search=False,\n",
    "                                       grid_param=params, default_param=params_default, fixed_params=fixed_params,\n",
    "                                       parallelize=False,\n",
    "                                       cv=False, replacement=True, n_iterations=1000, bootstrap_scores=True)\n",
    "\n",
    "# Running bootstrap estimation:\n",
    "boot_estimations.run(train_inputs=df_train.drop(drop_vars, axis=1),\n",
    "                     train_output=df_train['y'],\n",
    "                     test_inputs=df_test.drop(drop_vars, axis=1),\n",
    "                     test_output=df_test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsvenv",
   "language": "python",
   "name": "fsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
